{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class LeaguepediaScraper:\n",
    "    \"\"\"Class to scrape data from Leaguepedia website regarding professional LOL players\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.url_template = \"https://lol.fandom.com/wiki/{page}\"\n",
    "    \n",
    "    def get_page(self, page):\n",
    "        \"\"\"Gets the HTML of the page\"\"\"\n",
    "        url_prefix = \"lol.fandom.com/wiki/\"\n",
    "       # print(\"get_page:\",page)\n",
    "        # Find the index of url_prefix in the page\n",
    "        url_prefix_index = page.find(url_prefix)\n",
    "        # If url_prefix is not found, use it as it is\n",
    "        if url_prefix_index == -1:\n",
    "            url = self.url_template.format(page=page)\n",
    "        else:\n",
    "            # If url_prefix is found, remove it\n",
    "            url = self.url_template.format(page=page[url_prefix_index+len(url_prefix):])\n",
    "        #print(\"get_page:\",url)\n",
    "        response = requests.get(url)\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    def special_search(self, search_term:str) -> BeautifulSoup:\n",
    "        \"\"\"Searches for a player using the query in the search bar and returns the soup object of the page\"\"\"\n",
    "        url = self.url_template.format(page=f\"Special:Search?query={search_term}&scope=internal&navigationSearch=true\")\n",
    "        #print(\"Special Search:\",search_term, url)\n",
    "        response = requests.get(url)\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "    \n",
    "    def search_pro_page(self, summoner_name:str, given_name:str, family_name:str, region:str=None, soup:BeautifulSoup=None):\n",
    "        \"\"\"Finds the page of the professional player, given their summoner name, legal name, and region \n",
    "        returns the soup object\"\"\"\n",
    "        # First try the summoner name\n",
    "        if soup == None:\n",
    "            soup = self.get_page(summoner_name)\n",
    "        page_type = self.check_page(soup)\n",
    "\n",
    "        if page_type == \"disambiguation\":\n",
    "            new_url = self.find_disambiguation(soup, summoner_name, given_name, family_name)\n",
    "            # print(page_type,new_url)\n",
    "            return self.search_pro_page(summoner_name, given_name, family_name, region, self.get_page(new_url))\n",
    "        \n",
    "        elif page_type == \"doesn't exist\":\n",
    "            # Try changing the summoner name to title case\n",
    "            soup = self.get_page(summoner_name.title())\n",
    "            \n",
    "            if self.check_page(soup) != \"article\":\n",
    "                search_soup = self.special_search(summoner_name)\n",
    "                # Use the find_disambiguation function to find the correct link\n",
    "                new_url = self.find_disambiguation(search_soup, summoner_name, given_name, family_name)\n",
    "                \n",
    "                # If the new_url is None, use the find_search function\n",
    "                \n",
    "                # print(page_type,new_url)\n",
    "                return self.search_pro_page(summoner_name, given_name, family_name, region, self.get_page(new_url))\n",
    "        \n",
    "        page_type = self.check_page(soup)\n",
    "        if page_type == \"article\":\n",
    "            return soup\n",
    "        # If not found, try the given name and family name\n",
    "    \n",
    "    def find_disambiguation(self, soup:BeautifulSoup, summoner_name:str, first_name:str, family_name:str, search:bool=False):\n",
    "        \"\"\"Given a soup object containing a disambiguation page, find the correct link\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): the soup object for the disambiguation page\n",
    "            first_name (str): the player's first name\n",
    "            family_name (str): the player's family name\n",
    "        \"\"\"\n",
    "        #print(summoner_name, first_name, family_name)\n",
    "        result = None\n",
    "        for link in soup.find_all(\"a\"):\n",
    "            # If the link contains summoner_name.title() or summoner_name.lower() and first_name or family_name\n",
    "            # return the href\n",
    "            tests = [summoner_name.title(), summoner_name.lower(), summoner_name.upper(), summoner_name]\n",
    "            if any([s in link.text for s in tests]):\n",
    "                if (first_name in link.text or family_name in link.text):\n",
    "                    # Return the href\n",
    "                    result = link.get(\"href\")[6:]\n",
    "                    #print(result)\n",
    "                else:\n",
    "                    # Print the contents of the link after \"wiki/\"\n",
    "                    if len(summoner_name) == len(link.get(\"href\").split(\"wiki/\")[-1]):\n",
    "                        result = link.get(\"href\")\n",
    "        #print(\"Disambiguation: \",result)\n",
    "        return result\n",
    "            \n",
    "                \n",
    "        \n",
    "    def check_page(self, soup):\n",
    "        # Check if the page is an article, a disambiguation page, or a \"doesn't exist\" page\n",
    "        \n",
    "        # If it's an article, return \"article\"\n",
    "        article_text = \"Background Information\"\n",
    "        \n",
    "        # If it's a disambiguation page, return \"disambiguation\"\n",
    "        disambiguation_text = \"This disambiguation page lists articles associated with the same title.\"\n",
    "        \n",
    "        # If it's a \"doesn't exist\" page, return \"doesn't exist\"\n",
    "        doesnt_exist_text = \"There is currently no text in this page.\"\n",
    "        \n",
    "        if doesnt_exist_text in soup.text:\n",
    "            return \"doesn't exist\"\n",
    "        elif disambiguation_text in soup.text:\n",
    "            return \"disambiguation\"\n",
    "        elif article_text in soup.text:\n",
    "            return \"article\" \n",
    "        \n",
    "        # If it's something else, return \"other\"\n",
    "        return \"other\"\n",
    "    \n",
    "    def parse_pro_soloq_ids(self, soup:BeautifulSoup, region:str=None):\n",
    "        # Go through the tables and look for <td class=\"infobox-label\">Soloqueue IDs</td>\n",
    "        tables = soup.find_all(\"table\", class_=\"infobox\")\n",
    "        # If tables doesn't exist, return an empty dictionary\n",
    "        if tables == [] or tables == None:\n",
    "            return {}\n",
    "        \n",
    "        for table in tables:\n",
    "            for row in table.find_all(\"tr\"):\n",
    "                # If row.text starts with \"Soloqueue IDs\", save the row\n",
    "                search_string = \"Soloqueue IDs\"\n",
    "                if row.text.startswith(\"Soloqueue IDs\"):\n",
    "                    break\n",
    "                \n",
    "        # Get the contents inside all the <b> tag in the rows\n",
    "        server = row.find_all(\"b\")\n",
    "        # Iterate through all the server names, and get the text following them in the row variable\n",
    "        ids = {}\n",
    "        # Zip the two lists together into a dictionary\n",
    "        for s in server:\n",
    "            ids[s.text] = s.next_sibling.strip().split(\", \")\n",
    "            \n",
    "        if ids == {}:\n",
    "            # If the ids are not found, try a different format bc leaguepedia has inconsistent formatting\n",
    "\n",
    "            # Find the index of the search_string, and print everything after that\n",
    "            search_string_index = row.text.find(search_string)\n",
    "            \n",
    "            # If search_string is not found, return an empty dictionary\n",
    "            if search_string_index == -1:\n",
    "                return {}\n",
    "            \n",
    "            # Split the string by comma\n",
    "            raw_ids = row.text[search_string_index + len(search_string):].split(\", \")\n",
    "\n",
    "            # Iterate backwards through the list, and if the id doesn't contain a region parenthesis,\n",
    "            # add the parenthesis from the previous id.\n",
    "            # If it's the first id, add the region to the end of the id\n",
    "            for i in range(len(raw_ids)-1, -1, -1):\n",
    "\n",
    "                if \"(\" not in raw_ids[i]:\n",
    "                    try:\n",
    "                        # Find the index of the parenthesis in the previous id\n",
    "                        previous_id = raw_ids[i+1]\n",
    "                        parenthesis_index = previous_id.find(\"(\")\n",
    "                        # Add the previous id to the current id\n",
    "                        previous_region = previous_id[parenthesis_index:]\n",
    "                    except IndexError:\n",
    "                        previous_region = f\"({region})\"\n",
    "                    raw_ids[i] = raw_ids[i] + \" \" + previous_region\n",
    "\n",
    "            # For each entry in raw_ids, split by parenthesis\n",
    "            for raw_id in raw_ids:\n",
    "                raw_id = raw_id.split(\"(\")\n",
    "                server = raw_id[1].strip(\")\")\n",
    "                id = raw_id[0].strip()\n",
    "                # Append the server: id to the ids dictionary\n",
    "                if server in ids:\n",
    "                    ids[server].append(id)\n",
    "                else:\n",
    "                    ids[server] = [id]\n",
    "        return ids\n",
    "    \n",
    "    def get_pro_role(self, soup:BeautifulSoup) -> str:\n",
    "        \"\"\"Takes in the soup object of the pro player's page and returns their role\n",
    "        \n",
    "        Args: soup (BeautifulSoup): the soup object of the pro player's page\n",
    "        \n",
    "        Returns:\n",
    "            str: the pro player's role\n",
    "        \"\"\"\n",
    "        # Find all tables with the class \"infobox\"\n",
    "        tables = soup.find_all(\"table\", class_=\"infobox\")\n",
    "        # Locate the table with this attribute, then find the text of the next sibling\n",
    "        for table in tables:\n",
    "            for row in table.find_all(\"tr\"):\n",
    "                if row.text.startswith(\"Role\"):\n",
    "                    # Find the span class with class \"sprite\" and get its title\n",
    "                    role = row.find(\"span\", class_=\"sprite\").get(\"title\")\n",
    "                    return role\n",
    "        \n",
    "    \n",
    "    def get_pro_soloq_ids(self, summoner_name:str, first_name:str, family_name:str, region:str=None) -> dict:\n",
    "        \"\"\"Finds the pro player's soloq ids and return it \n",
    "        as a dictionary containing the region as keys and the ids as a list of strings\n",
    "\n",
    "        Args:\n",
    "            summoner_name (str): the pro player's summoner name\n",
    "            first_name (str): the pro player's first/given name\n",
    "            family_name (str): the pro player's family name\n",
    "            region (str): the pro player's tournament region\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of the pro player's soloq ids with the region as keys and the ids as a list of strings\n",
    "        \"\"\"\n",
    "        soup = self.search_pro_page(str(summoner_name), first_name.title(), family_name.title())\n",
    "        if soup == None:\n",
    "            raise Exception(\"Soup is None\")\n",
    "        ids = {}\n",
    "        role = (self.get_pro_role(soup))\n",
    "        non_player_roles = [\"manager\", \"analyst\", \"coach\", \"caoch\", \"media\"]\n",
    "        # If the role contains keywords from non_player_roles, return the role\n",
    "        for r in non_player_roles:\n",
    "            if r in role.lower():\n",
    "                return {'ids':ids, 'Roles':role}\n",
    "        ids = self.parse_pro_soloq_ids(soup, region)\n",
    "        return {'ids':ids, 'Roles':role}\n",
    "\n",
    "test = LeaguepediaScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R4ven', 'Domagalski', 'Milosz', 'EMEA']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': {}, 'Roles': 'Top Laner'}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(problem)\n",
    "test.get_pro_soloq_ids(problem[0], problem[1], problem[2], problem[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reaad the RCD.xlsx file and save into pandas dataframe\n",
    "raw_data = pd.read_excel(\"RCD.xlsx\", sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Team</th>\n",
       "      <th>Official Summoner Name</th>\n",
       "      <th>Roles</th>\n",
       "      <th>Legal First Name</th>\n",
       "      <th>Legal Family Name</th>\n",
       "      <th>End Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BR</td>\n",
       "      <td>FLUXO</td>\n",
       "      <td>Baldan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Giovani</td>\n",
       "      <td>Baldan</td>\n",
       "      <td>2024-11-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BR</td>\n",
       "      <td>FLUXO</td>\n",
       "      <td>Forlin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Leonardo</td>\n",
       "      <td>Pereira</td>\n",
       "      <td>2024-11-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BR</td>\n",
       "      <td>FLUXO</td>\n",
       "      <td>Tay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rodrigo</td>\n",
       "      <td>Panisa</td>\n",
       "      <td>2023-11-20 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BR</td>\n",
       "      <td>FLUXO</td>\n",
       "      <td>Disamis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pedro</td>\n",
       "      <td>Cavalcante</td>\n",
       "      <td>2024-11-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BR</td>\n",
       "      <td>FLUXO</td>\n",
       "      <td>Ancrath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rodrigo</td>\n",
       "      <td>Montrezol</td>\n",
       "      <td>2024-11-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>VN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emo</td>\n",
       "      <td>Sub/Mid</td>\n",
       "      <td>Vinh</td>\n",
       "      <td>Nguyen Thai</td>\n",
       "      <td>2026-11-16 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>VN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nugu</td>\n",
       "      <td>Mid</td>\n",
       "      <td>Dat</td>\n",
       "      <td>Tran Quoc</td>\n",
       "      <td>2026-11-16 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>VN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Slowz</td>\n",
       "      <td>Bot</td>\n",
       "      <td>Hung</td>\n",
       "      <td>Nguyen Huy</td>\n",
       "      <td>2026-11-16 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>VN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zodiac</td>\n",
       "      <td>Sup</td>\n",
       "      <td>Luong</td>\n",
       "      <td>Tieu Quoc</td>\n",
       "      <td>2026-11-16 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>VN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ryuk</td>\n",
       "      <td>Sub/Top</td>\n",
       "      <td>Khang</td>\n",
       "      <td>Vo Le Hoang</td>\n",
       "      <td>2024-11-18 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1350 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Region   Team Official Summoner Name    Roles Legal First Name  \\\n",
       "0      BR  FLUXO                 Baldan      NaN          Giovani   \n",
       "1      BR  FLUXO                 Forlin      NaN         Leonardo   \n",
       "2      BR  FLUXO                    Tay      NaN          Rodrigo   \n",
       "3      BR  FLUXO                Disamis      NaN            Pedro   \n",
       "4      BR  FLUXO                Ancrath      NaN          Rodrigo   \n",
       "..    ...    ...                    ...      ...              ...   \n",
       "84     VN    NaN                    Emo  Sub/Mid             Vinh   \n",
       "85     VN    NaN                   Nugu      Mid              Dat   \n",
       "86     VN    NaN                  Slowz      Bot             Hung   \n",
       "87     VN    NaN                 Zodiac      Sup            Luong   \n",
       "91     VN    NaN                   Ryuk  Sub/Top            Khang   \n",
       "\n",
       "   Legal Family Name             End Date  \n",
       "0             Baldan  2024-11-18 00:00:00  \n",
       "1            Pereira  2024-11-18 00:00:00  \n",
       "2             Panisa  2023-11-20 00:00:00  \n",
       "3         Cavalcante  2024-11-18 00:00:00  \n",
       "4          Montrezol  2024-11-18 00:00:00  \n",
       "..               ...                  ...  \n",
       "84       Nguyen Thai  2026-11-16 00:00:00  \n",
       "85         Tran Quoc  2026-11-16 00:00:00  \n",
       "86        Nguyen Huy  2026-11-16 00:00:00  \n",
       "87         Tieu Quoc  2026-11-16 00:00:00  \n",
       "91       Vo Le Hoang  2024-11-18 00:00:00  \n",
       "\n",
       "[1350 rows x 7 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = raw_df.copy()\n",
    "# Merge the Legal Given Name and Legal First Name column\n",
    "#df[\"Legal First Name\"] = df[\"Legal Given Name\"].fillna(df[\"Legal First Name\"])\n",
    "\n",
    "# Rename the \"End Date (Month, Day, Year)\" to just \"End Date\"\n",
    "df = df.rename(columns={\"End Date (Month, Day, Year)\":\"End Date\"})\n",
    "# Merge the \"Main Role\", \"Position\", and \"Positon\" columns\n",
    "df[\"Roles\"] = df[\"Main Role\"].fillna(df[\"Position\"]).fillna(df[\"Positon\"])\n",
    "df[\"Roles\"] = df[\"Roles\"].apply(lambda x: x.strip().title() if isinstance(x, str) else \"NaN\")\n",
    "\n",
    "# Manually change the roles of some players\n",
    "\n",
    "# Change the role of \"Freizer\" from NaN to \"Coach\"\n",
    "df.loc[df[\"Official Summoner Name\"] == \"Freizer\", \"Roles\"] = \"Coach\"\n",
    "\n",
    "# Change Duall's Official Summoner Name to \"DuaLL\"\n",
    "df.loc[df[\"Official Summoner Name\"] == \"Duall\", \"Official Summoner Name\"] = \"DuaLL\"\n",
    "\n",
    "# Change Beanovich's First Name to \"Louis\" since they mistyped it as \"Loius\"\n",
    "df.loc[df[\"Official Summoner Name\"] == \"Beanovich\", \"Legal First Name\"] = \"Louis\"\n",
    "\n",
    "# Filter out all players whose \"Roles\" contains the word \"Coach\", \"Manager\", or \"Analyst\"\n",
    "df = df[~df[\"Roles\"].str.contains(\"Coach|Manager|Analyst|Caoch\")]\n",
    "\n",
    "roleDict = {\"Toplaner\": \"Top\", \"Jungler\": \"Jungle\", \"Midlaner\": \"Mid\", \"Botlaner\": \"Bot\", \"Support\": \"Sup\",\n",
    "            \"Jun\": \"Jungle\", \"Jug\":\"Jungle\", \"Jung\": \"Jungle\",\n",
    "            \"Middle\" : \"Mid\",\n",
    "            \"Adc\": \"Bot\", \"Bottom\": \"Bot\"}\n",
    "# If the role is in the roleDict, replace it with the value in the roleDict\n",
    "df[\"Roles\"] = df[\"Roles\"].replace(roleDict)\n",
    "df = df[[\"Region\", \"Team\", \"Official Summoner Name\",\"Roles\" ,\"Legal First Name\", \"Legal Family Name\", \"End Date\"]]\n",
    "\n",
    "df = df.dropna(subset=[\"Official Summoner Name\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"pro_scrape_df.csv\"):\n",
    "    already_scraped = []\n",
    "    ids = []\n",
    "    roles = []\n",
    "else:\n",
    "    pro_scrape_df = pd.read_csv(\"pro_scrape_df.csv\")\n",
    "    already_scraped = list(pro_scrape_df[\"Official Summoner Name\"])\n",
    "    ids = list(pro_scrape_df[\"ids\"])\n",
    "    roles = list(pro_scrape_df[\"Roles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df[\"Official Summoner Name\"].isin(already_scraped)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1350, 160, 160, 160)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(ids), len(roles), len(already_scraped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Take the first 160 rows of the dataframe and append the ids and roles to the lists\n",
    "test_df = df.head(160).copy()\n",
    "test_df[\"Scrapped Roles\"] = roles\n",
    "test_df[\"Alts\"] = ids\n",
    "test_df\n",
    "# Save test_df to a csv file named \"test_alt_df.csv\"\n",
    "test_df.to_csv(\"test_alt_df.csv\", index=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776d252db2844b7a901029227202629e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region                              EMEA\n",
      "Team                              REBELS\n",
      "Official Summoner Name         Beanovich\n",
      "Roles                                Bot\n",
      "Legal First Name                   Louis\n",
      "Legal Family Name         Joscha Schmitz\n",
      "End Date                             NaT\n",
      "Name: 355, dtype: object\n",
      "['Beanovich', 'Joscha Schmitz', 'Louis', 'EMEA']\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[224], line 10\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m     result \u001b[39m=\u001b[39m test\u001b[39m.\u001b[39;49mget_pro_soloq_ids(row[\u001b[39m\"\u001b[39;49m\u001b[39mOfficial Summoner Name\u001b[39;49m\u001b[39m\"\u001b[39;49m], row[\u001b[39m\"\u001b[39;49m\u001b[39mLegal First Name\u001b[39;49m\u001b[39m\"\u001b[39;49m], row[\u001b[39m\"\u001b[39;49m\u001b[39mLegal Family Name\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     11\u001b[0m     ids\u001b[39m.\u001b[39mappend(result[\u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[220], line 212\u001b[0m, in \u001b[0;36mLeaguepediaScraper.get_pro_soloq_ids\u001b[1;34m(self, summoner_name, first_name, family_name, region)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Finds the pro player's soloq ids and return it \u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39mas a dictionary containing the region as keys and the ids as a list of strings\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39m    dict: A dictionary of the pro player's soloq ids with the region as keys and the ids as a list of strings\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m soup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearch_pro_page(\u001b[39mstr\u001b[39;49m(summoner_name), first_name\u001b[39m.\u001b[39;49mtitle(), family_name\u001b[39m.\u001b[39;49mtitle())\n\u001b[0;32m    213\u001b[0m \u001b[39mif\u001b[39;00m soup \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[220], line 60\u001b[0m, in \u001b[0;36mLeaguepediaScraper.search_pro_page\u001b[1;34m(self, summoner_name, given_name, family_name, region, soup)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[39m# If the new_url is None, use the find_search function\u001b[39;00m\n\u001b[0;32m     58\u001b[0m         \n\u001b[0;32m     59\u001b[0m         \u001b[39m# print(page_type,new_url)\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_pro_page(summoner_name, given_name, family_name, region, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_page(new_url))\n\u001b[0;32m     62\u001b[0m page_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_page(soup)\n",
      "Cell \u001b[1;32mIn[220], line 16\u001b[0m, in \u001b[0;36mLeaguepediaScraper.get_page\u001b[1;34m(self, page)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m# print(\"get_page:\",page)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m  \u001b[39m# Find the index of url_prefix in the page\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m  url_prefix_index \u001b[39m=\u001b[39m page\u001b[39m.\u001b[39;49mfind(url_prefix)\n\u001b[0;32m     17\u001b[0m  \u001b[39m# If url_prefix is not found, use it as it is\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[224], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m problem \u001b[39m=\u001b[39m [row[\u001b[39m\"\u001b[39m\u001b[39mOfficial Summoner Name\u001b[39m\u001b[39m\"\u001b[39m], row[\u001b[39m\"\u001b[39m\u001b[39mLegal Family Name\u001b[39m\u001b[39m\"\u001b[39m], row[\u001b[39m\"\u001b[39m\u001b[39mLegal First Name\u001b[39m\u001b[39m\"\u001b[39m], row[\u001b[39m\"\u001b[39m\u001b[39mRegion\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(problem)\n\u001b[1;32m---> 19\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Error"
     ]
    }
   ],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "# Iterate through each row in the dataframe and get the soloq ids by calling the get_pro_soloq_ids function on the Summoner Name, Legal First Name, and Legal Family Name columns\n",
    "for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    if index < len(already_scraped) - 1:\n",
    "        continue\n",
    "    if row[\"Official Summoner Name\"] in already_scraped:\n",
    "        continue\n",
    "    #print(row[\"Official Summoner Name\"], row[\"Legal Family Name\"], row[\"Legal First Name\"])\n",
    "    try:\n",
    "        result = test.get_pro_soloq_ids(row[\"Official Summoner Name\"], row[\"Legal First Name\"], row[\"Legal Family Name\"])\n",
    "        ids.append(result['ids'])\n",
    "        roles.append(result['Roles'])\n",
    "        already_scraped.append(row[\"Official Summoner Name\"])\n",
    "        #print(row[\"Official Summoner Name\"], row[\"Legal Family Name\"], row[\"Legal First Name\"], result['Roles'])\n",
    "    except:\n",
    "        print(row)\n",
    "        problem = [row[\"Official Summoner Name\"], row[\"Legal Family Name\"], row[\"Legal First Name\"], row[\"Region\"]]\n",
    "        print(problem)\n",
    "        raise Exception(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take already_scraped, ids, roles and add them to the dataframe\n",
    "pro_scrape_df = pd.DataFrame({\"Official Summoner Name\":already_scraped, \"ids\":ids, \"Roles\":roles})\n",
    "pro_scrape_df.to_csv(\"pro_scrape_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
